# -*- coding:UTF-8 -*-
import gc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from sklearn import preprocessing
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split


def feature_Engineering(train_validate_df_org, test_df_org):
    '''
    #检查是否有缺失数据
    for i in range(0, train_validate_df_org.shape[1]):
        if False != train_validate_df_org.isnull().any().values[i]:
            print(train_validate_df_org.isnull().any().keys()[i] + ' have null')
    '''
    '''
    #绘制预测值比例
    train_validate_df_org['target'].value_counts().plot.pie(autopct = '%1.2f%%')
    '''
    
    train_validate_X = train_validate_df_org.ix[:, 2:]
    train_validate_Y = train_validate_df_org.ix[:, 1:2]
    test_X = test_df_org.ix[:, 1:]
    
    #combined_train_validate_test_X = train_validate_df_org.ix[:, 2:].append(test_df_org.ix[:, 1:])
    test_length = test_df_org.shape[0]
    train_validate_length = train_validate_df_org.shape[0]
    feature_nums = train_validate_X.shape[1]
    
    # StandardScaler will subtract the mean from each value then scale to the unit variance    
    scaler = preprocessing.StandardScaler()
    for i in range(0, feature_nums):
        train_validate_X[train_validate_X.columns[i]] = scaler.fit_transform(train_validate_X[train_validate_X.columns[i]].values.reshape(-1, 1))
        test_X[test_X.columns[i]] = scaler.fit_transform(test_X[test_X.columns[i]].values.reshape(-1, 1))
    '''
    train_validate_X = combined_train_validate_test_X[0: train_validate_length]
    train_validate_Y = train_validate_df_org.ix[:, 1:2]
    test_X = combined_train_validate_test_X[train_validate_length: ]
    '''
    '''
    #特征选择：卡方检验
    drop_pvalue_stand = 0.05
    train_validate_X_scaled = train_validate_X + train_validate_X.min().apply(lambda x: 0 if x > 0 else abs(x)) #转换成正数,如果最小值小于0，则加上最小值得绝对值
    feature_model = SelectKBest(chi2, k = len(train_validate_X_scaled.ix[0,:]))#选择k个最佳特征
    feature_model.fit_transform(train_validate_X_scaled, train_validate_Y)#data_X是特征数据，data_Y是标签数据，该函数可以选择出k个特征
    scores = feature_model.scores_  #得分,越大说明越相关
    pvalues = feature_model.pvalues_  #p-values,越小越相关  若p值小于0.05，则可以推翻H0（两者没有关系），推出H1（两者有关系）。
    drop_feature_nums = 0
    drop_columns = []
    for i in range(0, len(pvalues)):
        if drop_pvalue_stand < pvalues[i]:
            drop_feature_nums+=1
            drop_columns.append(train_validate_X_scaled.columns[i])
            print('drop feature: ' + train_validate_X_scaled.columns[i] + ' drop_feature_num: ' + str(drop_feature_nums))
    train_validate_X.drop(drop_columns, axis=1, inplace = True)
    test_X.drop(drop_columns, axis=1, inplace = True)
    '''
    
    train_data_X, validate_data_X, train_data_Y, validate_data_Y = train_test_split(np.array(train_validate_X), np.array(train_validate_Y), test_size=0.1, random_state=7)
    
    return train_data_X, train_data_Y, validate_data_X, validate_data_Y, np.array(test_X)


def feature_Engineering1(train_validate_df_org, test_df_org):
    test_length = test_df_org.shape[0]
    train_validate_length = train_validate_df_org.shape[0]
    
    train_validate_Y = train_validate_df_org.pop('target')
    train_validate_ids = train_validate_df_org.pop('ID_code')
    test_ids = test_df_org.pop('ID_code')
    
    combined_train_validate_test_X = pd.concat([train_validate_df_org, test_df_org])
    
    original_features = combined_train_validate_test_X.columns
    feature_nums = combined_train_validate_test_X.shape[1]
    
    del train_validate_df_org, test_df_org
    gc.collect()
    
    train_validate_X = train_validate_df_org.ix[:, 2:]
    train_validate_Y = train_validate_df_org.ix[:, 1:2]
    test_X = test_df_org.ix[:, 1:]
    
    merged = pd.concat([train_validate_X, test_X])
    
    #combined_train_validate_test_X = train_validate_df_org.ix[:, 2:].append(test_df_org.ix[:, 1:])

    
    # StandardScaler will subtract the mean from each value then scale to the unit variance    
    scaler = preprocessing.StandardScaler()
    for i in range(0, feature_nums):
        train_validate_X[train_validate_X.columns[i]] = scaler.fit_transform(train_validate_X[train_validate_X.columns[i]].values.reshape(-1, 1))
        test_X[test_X.columns[i]] = scaler.fit_transform(test_X[test_X.columns[i]].values.reshape(-1, 1))
    '''
    train_validate_X = combined_train_validate_test_X[0: train_validate_length]
    train_validate_Y = train_validate_df_org.ix[:, 1:2]
    test_X = combined_train_validate_test_X[train_validate_length: ]
    '''
    '''
    #特征选择：卡方检验
    drop_pvalue_stand = 0.05
    train_validate_X_scaled = train_validate_X + train_validate_X.min().apply(lambda x: 0 if x > 0 else abs(x)) #转换成正数,如果最小值小于0，则加上最小值得绝对值
    feature_model = SelectKBest(chi2, k = len(train_validate_X_scaled.ix[0,:]))#选择k个最佳特征
    feature_model.fit_transform(train_validate_X_scaled, train_validate_Y)#data_X是特征数据，data_Y是标签数据，该函数可以选择出k个特征
    scores = feature_model.scores_  #得分,越大说明越相关
    pvalues = feature_model.pvalues_  #p-values,越小越相关  若p值小于0.05，则可以推翻H0（两者没有关系），推出H1（两者有关系）。
    drop_feature_nums = 0
    drop_columns = []
    for i in range(0, len(pvalues)):
        if drop_pvalue_stand < pvalues[i]:
            drop_feature_nums+=1
            drop_columns.append(train_validate_X_scaled.columns[i])
            print('drop feature: ' + train_validate_X_scaled.columns[i] + ' drop_feature_num: ' + str(drop_feature_nums))
    train_validate_X.drop(drop_columns, axis=1, inplace = True)
    test_X.drop(drop_columns, axis=1, inplace = True)
    '''
    
    train_data_X, validate_data_X, train_data_Y, validate_data_Y = train_test_split(np.array(train_validate_X), np.array(train_validate_Y), test_size=0.1, random_state=7)
    
    return train_data_X, train_data_Y, validate_data_X, validate_data_Y, np.array(test_X)

def build_model(train_data_X, train_data_Y):
    # build the model
    print('Build model...')
    optimizer = optimizers.Adam(lr=0.001)
    model = Sequential()
    #model.add(BatchNormalization(input_shape = (train_data_X.shape[1],)))
    model.add(Dense(256, input_shape = (train_data_X.shape[1],), activation='selu'))
    model.add(Dropout(0.2))
    model.add(Dense(256, activation='selu'))
    model.add(Dropout(0.2))
    model.add(Dense(256, activation='selu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    model.fit(train_data_X, train_data_Y, batch_size=32, epochs=10)
    
    return model


if __name__ == '__main__':
    train_validate_df_org = pd.read_csv('./train.csv')
    test_df_org = pd.read_csv('./test.csv')
    ID_code = test_df_org['ID_code']
    
    #特征工程处理
    train_data_X, train_data_Y, validate_data_X, validate_data_Y, test_data_X = feature_Engineering1(train_validate_df_org, test_df_org)
    
    model = build_model(train_data_X, train_data_Y)
    
    print(model.evaluate(train_data_X, train_data_Y))
    print(model.evaluate(validate_data_X, validate_data_Y))

    test_data_Y = model.predict_classes(test_data_X)
    pd.DataFrame({"ID_code": ID_code, "target": test_data_Y.reshape(-1)}).to_csv('test_Y.csv', index = False, header = True)
    
    print()