# -*- coding: utf-8 -*-
#! /usr/bin/env python
import tensorflow as tf
import numpy as np
import os

'''
分布式计算, in_graph模式
'''

# 定义常量，用于创建数据流图
flags = tf.app.flags
# 参数服务器主机
flags.DEFINE_string("ps_hosts","192.168.43.205:1234", "Comma-separated list of hostname:port pairs")
# 工作节点主机
flags.DEFINE_string("worker_hosts", "192.168.43.206:1235,192.168.43.207:1236", "Comma-separated list of hostname:port pairs")
# 本作业是工作节点还是参数服务器
flags.DEFINE_string("job_name", None,"job name: worker or ps")
# task_index从0开始。0代表用来初始化变量的第一个任务
flags.DEFINE_integer("task_index", None, "Worker task index, should be >= 0. task_index=0 is the master worker task the performs the variable initialization ")
FLAGS = flags.FLAGS


def main(unused_argv):
  if FLAGS.job_name is None or FLAGS.job_name == "":
    raise ValueError("Must specify an explicit `job_name`")
  if FLAGS.task_index is None or FLAGS.task_index =="":
    raise ValueError("Must specify an explicit `task_index`")

  print("job name = %s" % FLAGS.job_name)
  print("task index = %d" % FLAGS.task_index)
  #Construct the cluster and start the server
  # 读取集群描述信息
  ps_spec = FLAGS.ps_hosts.split(",")
  worker_spec = FLAGS.worker_hosts.split(",")
  
  # Get the number of workers.
  num_workers = len(worker_spec)
  # 创建TensorFlow集群描述对象
  cluster = tf.train.ClusterSpec({"ps": ps_spec, "worker": worker_spec})
  
  server = tf.train.Server(cluster, job_name = FLAGS.job_name, task_index=FLAGS.task_index)
  # 如果是参数服务器，直接启动即可。这里，进程就会阻塞在这里
  # 下面的tf.train.replica_device_setter代码会将参数批定给ps_server保管
  if "ps" == FLAGS.job_name || ("worker" == FLAGS.job_name && "0" == FLAGS.task_index):
    server.join()


'''
(3) 创建网络结构
'''
#设定训练集数据长度
n_train = 100

#生成x数据，[-1,1]之间，均分成n_train个数据
train_x = np.linspace(-1,1,n_train).reshape(n_train,1)

#把x乘以2，在加入(0,0.3)的高斯正太分布
train_y = 2*train_x + np.random.normal(loc=0.0,scale=0.3,size=[n_train,1])

#创建网络结构时，通过tf.device()函数将全部的节点都放在当前任务下
#with tf.device(tf.train.replica_device_setter(worker_device = '/job:worker/task:{0}'.format(task_index), cluster = cluster_spec)):
with tf.device("/job:ps/task:0"):
    '''
    前向反馈
    '''
    #创建占位符
    input_x = tf.placeholder(dtype=tf.float32)
    input_y = tf.placeholder(dtype=tf.float32)
    
    
    #模型参数
    w = tf.Variable(tf.truncated_normal(shape=[1],mean=0.0,stddev=1),name='w')    #设置正太分布参数  初始化权重
    b = tf.Variable(tf.truncated_normal(shape=[1],mean=0.0,stddev=1),name='b')    #设置正太分布参数  初始化偏置
    
    w1 = tf.Variable(tf.truncated_normal(shape=[1],mean=0.0,stddev=1),name='w1')    #设置正太分布参数  初始化权重
    b1 = tf.Variable(tf.truncated_normal(shape=[1],mean=0.0,stddev=1),name='b1')    #设置正太分布参数  初始化偏置
    
    #创建一个global_step变量
    global_step = tf.train.get_or_create_global_step()
    
with tf.device("/job:worker/task:0"):    
    #前向结构
    pred1 = tf.multiply(w,input_x) + b

with tf.device("/job:worker/task:1"):    
    #前向结构
    pred = tf.multiply(w1,pred1) + b1
    
    '''
    反向传播bp
    '''
    #定义代价函数  选取二次代价函数
    cost = tf.reduce_mean(tf.square(input_y - pred))
    
    #设置求解器 采用梯度下降法 学习了设置为0.001 并把global_step变量放到优化器中，这样每运行一次优化器，global_step就会自动获得当前迭代的次数
    train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost,global_step = global_step)
    
sess = tf.Session(target=server.target, config=tf.ConfigProto(log_device_placement=True))

print("Parameter server: waiting for cluster connection...")
sess.run(tf.report_uninitialized_variables())
print("Parameter server: cluster ready!")

print("Parameter server: initializing variables...")
sess.run(tf.global_variables_initializer())
print("Parameter server: variables initialized")

training_epochs = 2000
display_step = 20

print("sess ok：")
print(global_step.eval(session=sess))
print('开始迭代：')
     
#存放批次值和代价值
plotdata = {'batch_size':[],'loss':[]}

#开始迭代 这里step表示当前执行步数，迭代training_epochs轮  需要执行training_epochs*n_train步
for step in range(training_epochs*n_train):
    for (x,y) in zip(train_x,train_y):
        #开始执行图  并返回当前步数
        _,step = sess.run([train,global_step],feed_dict={input_x:x,input_y:y})
        
         #一轮训练完成后 打印输出信息
        if step % display_step == 0:
            #计算代价值
            loss = sess.run(cost,feed_dict={input_x:train_x,input_y:train_y})
            print('step {0}  cost {1}  w {2}  b{3}'.format(step,loss,sess.run(w),sess.run(b)))
    
            #保存每display_step轮训练后的代价值以及当前迭代轮数
            if not loss == np.nan:
                plotdata['batch_size'].append(step)
                plotdata['loss'].append(loss)
            
    
print('Finished!')


if __name__ == "__main__":
  tf.app.run()




